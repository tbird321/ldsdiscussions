You are working in the repository at /work/71f685a3-1103-4abc-89c4-f3242f481ca6/repo on branch 'scrape-ldsdiscussions-20251223-213148'.

Your task is to scrape HTML content from the website https://www.ldsdiscussions.com/ and save it to a GitHub repository.

**Step 1: Create a plan file**
- Create a file called scraper-plan.json in the working directory (/work/71f685a3-1103-4abc-89c4-f3242f481ca6/repo/)
- This file will track which pages have been downloaded
- If the file doesn't exist, you need to create it with an empty list: {"pages": [], "downloaded": []}

**Step 2: Read the home page and find all links**
- If scraper-plan.json has an empty pages list, you need to:
  1. Download the home page HTML from https://www.ldsdiscussions.com/
  2. Parse the HTML to find all anchor tags (<a> tags) that link to pages on the same domain (ldsdiscussions.com)
  3. Extract the href attributes from these links
  4. Normalize the URLs (handle relative URLs, remove fragments, etc.)
  5. Add all unique URLs to the pages array in scraper-plan.json
  6. Mark all pages as "downloaded": false initially

**Step 3: Download HTML content**
- Create a folder called content in the working directory if it doesn't exist
- Look at scraper-plan.json to find pages that are marked as downloaded: false
- Download the HTML content of up to 5 new pages (pages not yet downloaded)
- For each page:
  - Download the HTML content using curl or wget
  - Save it to the content folder with a filename based on the URL path (sanitize the filename)
  - Update scraper-plan.json to mark that page as downloaded: true
  - Add a timestamp for when it was downloaded

**Step 4: Commit the changes**
- After downloading 5 new files, commit all changes with the message: "Scrape 5 pages from ldsdiscussions.com"

**Important Notes:**
- Use Python or a shell script to parse HTML and extract links
- Handle relative URLs correctly (they should be converted to absolute URLs)
- Only process links that point to pages on ldsdiscussions.com (same domain)
- Skip external links, mailto links, anchor links (#), and javascript: links
- Create meaningful filenames from URLs (e.g., /book-of-mormon/ -> ook-of-mormon.html)
- Ensure the content folder structure is organized

**Tools available:**
- You can use Python with BeautifulSoup or html.parser to parse HTML
- You can use curl or wget to download pages
- You can use git commands to commit changes
- You can read/write JSON files using Python's json module

Begin by checking if scraper-plan.json exists, and proceed with the steps above.